{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "# Random Fourier features class\n",
    "class RFFKernel(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Approximates the feature map of a kernel by Monte Carlo approximation of \n",
    "    it's Fourier transform. Implements random Fourier features [1].\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    gamma : float\n",
    "        Parameter of the Gaussian kernel exp(-gamma * w^2)\n",
    "        \n",
    "    D : int\n",
    "        Number of Monte Carlo samples per feature\n",
    "    \n",
    "    [1] Rahimi, A., & Recht, B. (2008). Random features for large-scale kernel machines. \n",
    "        In Advances in neural information processing systems (pp. 1177-1184)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,  D=50):\n",
    "        self.D = D\n",
    "        self.is_fitted = False \n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Draws D samples for direction w and random offset b\n",
    "        \n",
    "        X : Data {array, matrix}, shape (n_samples, n_dimension) \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns the direction vector w, the offset b and the boolean\n",
    "            fitted.\n",
    "        \"\"\"\n",
    "        \n",
    "        dimension = X.shape[1] # dimension of the data\n",
    "        self.w_direction = np.random.normal(size=(self.D, dimension)) #*np.sqrt(2 * self.gamma)\n",
    "        self.b_offset = np.random.uniform(0, 2*np.pi, size=(1, self.D))\n",
    "        self.is_fitted = True\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _transform(self, X):\n",
    "        \"\"\"\n",
    "        Apply the approximate feature map to X.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : Data {array, matrix}, shape (n_samples, n_features)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Z : array of transformed features, shape (n_samples, n_components [D])\n",
    "        \"\"\"\n",
    "        \n",
    "        Xw = X.dot(self.w_direction.T)\n",
    "        \n",
    "        Z = np.sqrt(2 / self.D) * np.cos(Xw + self.b_offset)\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def approx_kernel(self, X, Y=None, gamma=1):\n",
    "        \"\"\"\n",
    "        Computes the kernel gram matrix using the transformed Fourier features\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : Data {array, matrix}, shape (n_samples, n_features)\n",
    "        \n",
    "        Y : Data {array, matrix}, shape (n_samples, n_features)\n",
    "        \n",
    "        gamma: lengthscale {float} \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        K : gram matrix (n_samples, n_samples)\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise NotFittedError('Must call .fit(X) before the kernel can be approximated.')\n",
    "                \n",
    "        Zx = self._transform(X) \n",
    "        if Y is not None:\n",
    "            Zy = self._transform(Y)\n",
    "            K = Zx.dot(Zy.T) * np.sqrt(2 * gamma)\n",
    "        else:\n",
    "            K = Zx.dot(Zx.T) * np.sqrt(2 * gamma)      \n",
    "\n",
    "        return K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KL Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(mu, cov):\n",
    "    \"\"\"\n",
    "    Computes the KL divergence between a multivariate normal with mean (mu) and \n",
    "    covariance (cov) and a standard multivariate normal N(0,I)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mu: mean of the posterior {array}, shape (n_samples + 1)\n",
    "\n",
    "    S: covariance of the posterior {matrix}, shape (n_samples + 1, n_samples + 1)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    KL : value of KL divergence {float}\n",
    "    \n",
    "    \"\"\"\n",
    "    n = mu.shape[0]\n",
    "    kl = 0.5 * (-np.log(np.linalg.det(cov)) - n + np.trace(cov) + np.dot(mu.T,mu))\n",
    "    return kl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Regression Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "class BR():\n",
    "    \"\"\"\n",
    "    Bayesian linear regression with radial basis functions where the kernel\n",
    "    matrix is approximated with random features.\n",
    "    \n",
    "    Parameters\n",
    "    ----------        \n",
    "    num_rff_samples : int\n",
    "        Number of Monte Carlo samples per feature\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_rff_samples=2000):\n",
    "        self.num_rff_samples = num_rff_samples\n",
    "        self.is_fitted = False \n",
    "        \n",
    "        # Instantiate RFF approximations to the design matrix\n",
    "        self.kernel = RFFKernel(D=self.num_rff_samples)\n",
    "        self.post_k_xs = RFFKernel(D=self.num_rff_samples)\n",
    "        self.pre_k_xs = RFFKernel(D=self.num_rff_samples)\n",
    "    \n",
    "    def add_data(self, X, Y):\n",
    "        \"\"\"\n",
    "        Adds new data to the regressor and fits the random features kernel.\n",
    "        \n",
    "        Parameters\n",
    "        ----------        \n",
    "        X : Data {array, matrix}, shape (n_samples, n_features)\n",
    "        \n",
    "        Y : Data {array, matrix}, shape (n_samples, n_features)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.kernel.fit(X) \n",
    "        self.is_fitted = True\n",
    "        \n",
    "    def calculate_posterior(self, gamma=0.5, var_y=0.2, var_w=1):\n",
    "        \"\"\"\n",
    "        Calculates the posterior over the weights given the data and the hyperparameters.\n",
    "        \n",
    "        Parameters\n",
    "        ----------        \n",
    "        gamma : {float} random feature lengthscale  \n",
    "        \n",
    "        var_y: {float} target variance \n",
    "        \n",
    "        var_w: {float} prior weight variance  \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mu: mean of the posterior {array}, shape (n_samples + 1)\n",
    "        \n",
    "        S: convariance of the posterior {matrix}, shape (n_samples + 1, n_samples + 1)\n",
    "        \n",
    "        phi: design matrix {matrix}, shape (n_samples, n_samples + 1)\n",
    "        \n",
    "        \"\"\"\n",
    "        self.var_y = var_y\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        if not self.is_fitted:\n",
    "            raise NotFittedError('Must call BR.fit() with new data first.')\n",
    "                 \n",
    "        phi = np.append(np.ones((self.X.shape[0],1)), \\\n",
    "                        self.kernel.approx_kernel(self.X, self.X, gamma=self.gamma), axis=1)# Append ones for bias\n",
    "        pre_w = 1/var_w * np.eye(len(self.X)+1) # prior covariance matrix\n",
    "        # Mean and covariance matrix for weights given x and y\n",
    "        self.S = np.linalg.inv((phi.T).dot(phi) / self.var_y + pre_w) # posterior distribution covariance matrix\n",
    "        self.mu = self.S.dot(phi.T).dot(self.Y) / self.var_y # MAP weights to use in mean(y*)\n",
    "\n",
    "        return self.mu, self.S, phi\n",
    "    \n",
    "    def optimise(self, maxiter=100, disp=False):\n",
    "        \"\"\"\n",
    "        Optimises the variational upper bound using Nelder-Mead method.\n",
    "        \n",
    "        Parameters\n",
    "        ----------        \n",
    "        maxiter : {int} maximum number of iterations of miminize \n",
    "        \n",
    "        disp: {bool} set to True to print convergence messages\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        gamma: {float} random feature lengthscale \n",
    "        \n",
    "        var_y: {float} target variance \n",
    "        \n",
    "        \"\"\"\n",
    "    \n",
    "        if not self.is_fitted:\n",
    "            raise NotFittedError('Must call BR.fit() before the kernel can be approximated.')\n",
    "        \n",
    "        opts={'maxiter': maxiter, 'disp': disp}\n",
    "        result = minimize(self._upper_bound, [0, 0], method=\"Nelder-Mead\", options=opts)\n",
    "        print(result.message)\n",
    "        return np.exp(result.x)\n",
    "    \n",
    "        \n",
    "    def _upper_bound(self, x0):\n",
    "        \"\"\"\n",
    "        Variational upper bound (VUB): -E[log p(Y|f(X))] + KL(q(w)||p(w))\n",
    "        where -E[log p(Y|f(X))] is the expectation of the Gaussian likelihood,\n",
    "        and KL(q(w)||p(w)) is the KL divergence between posterior and prior over\n",
    "        the weights.\n",
    "        \n",
    "        Parameters\n",
    "        ----------        \n",
    "        x0 : {array}, shape (2,) (gamma, var_y)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        VUB: {float} value of variational upper bound\n",
    "        \"\"\"   \n",
    "\n",
    "        gamma, var_y = np.exp(x0)\n",
    "        mu, S, phi = self.calculate_posterior(gamma=gamma, var_y=var_y)\n",
    "        kl = kl_divergence(mu, S) # KL divergence between posterior and prior on the weights\n",
    "        N = self.X.shape[0]\n",
    "        log_p = -(N/2)*np.log(2*np.pi*var_y) - (1/(2*var_y))*((self.Y - phi.dot(mu))**2).mean() #likelihood\n",
    "     \n",
    "        return -log_p + kl\n",
    "            \n",
    "    def sample_weights(self, num_samples=1):\n",
    "        \"\"\"\n",
    "        Draws a sample of the weights from the posterior over the weights.\n",
    "        \n",
    "        Parameters\n",
    "        ----------        \n",
    "        num_samples : {int} number of sets of weights to draw\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        weights : {array, matrix}, shape (n_samples + 1, num_samples)\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        return np.random.multivariate_normal(self.mu.squeeze(), self.S, num_samples)\n",
    "    \n",
    "    def post_one_step(self, xs, weights):\n",
    "        \"\"\"\n",
    "        Predict for new values xs given a particular set of weights\n",
    "        \n",
    "        Parameters\n",
    "        ----------        \n",
    "        xs : {float, array} x position(s) for prediction\n",
    "        \n",
    "        weights : {array} set of weights drawn from the posterior \n",
    "         \n",
    "        Returns\n",
    "        -------\n",
    "        ys : {float, array} predictions\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.post_k_xs.fit(xs)\n",
    "        phi_xs = np.append(np.ones((len(xs),1)),\n",
    "                           self.post_k_xs.approx_kernel(xs, self.X, self.gamma), axis=1)# Append ones for bias\n",
    "        ys = phi_xs.dot(weights.T)\n",
    "        \n",
    "        return ys\n",
    "    \n",
    "    def pred_one_step(self, xs):\n",
    "        \"\"\"\n",
    "        Predict for new values xs from the predictive distribution.\n",
    "        \n",
    "        Parameters\n",
    "        ----------        \n",
    "        xs : {float, array} x position(s) for prediction\n",
    "                 \n",
    "        Returns\n",
    "        -------\n",
    "        mu : {float, array} MAP of predictive, shape (len(xs),)\n",
    "        \n",
    "        stdev : {float, array} standard deviation of the predictive, shape (len(xs),) \n",
    "        \n",
    "        \"\"\"\n",
    "        self.pre_k_xs.fit(xs)\n",
    "        pred_phi_xs = np.append(np.ones((len(xs),1)), \n",
    "                                self.pre_k_xs.approx_kernel(xs, self.X, self.gamma), axis=1)# Append ones for bias\n",
    "        mu = pred_phi_xs.dot(self.mu) # calculate mean(y*)\n",
    "        stdev = (np.sum(pred_phi_xs.dot(self.S) * pred_phi_xs, axis = 1) + self.var_y) ** 0.5 # calculate Var(y*)^0.5\n",
    "                \n",
    "        return mu, stdev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os.path\n",
    "from scipy.io import loadmat\n",
    "from math import floor\n",
    "\n",
    "if not os.path.isfile('elevators.mat'):\n",
    "    print('Downloading \\'elevators\\' UCI dataset...')\n",
    "    urllib.request.urlretrieve('https://drive.google.com/uc?export=download&id=1jhWL3YUHvXIaftia4qeAyDwVxo6j1alk', 'elevators.mat')\n",
    "\n",
    "data = np.array(loadmat('elevators.mat')['data'])\n",
    "X = data[:, :-1]\n",
    "X = X - X.min(0)[0]\n",
    "X = 2 * (X / X.max(0)[0]) - 1\n",
    "y = data[:, -1]\n",
    "\n",
    "dataX = X[0:1000,0:10]\n",
    "datay = y[0:1000]\n",
    "\n",
    "train_n = int(floor(0.8*len(dataX)))\n",
    "\n",
    "train_x = dataX[:train_n, :]\n",
    "train_y = datay[:train_n]\n",
    "\n",
    "test_x = dataX[train_n:, :]\n",
    "test_y = datay[train_n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add data and optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: -2665.072824\n",
      "         Iterations: 91\n",
      "         Function evaluations: 173\n",
      "Optimization terminated successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([9.18458169e-06, 7.25443008e-05])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "br = BR()\n",
    "br.add_data(train_x, train_y)\n",
    "br.optimise(maxiter=120,disp=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicts outputs for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, stdev = br.pred_one_step(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 0.045809500546632444\n"
     ]
    }
   ],
   "source": [
    "print('Test MSE: {}'.format(((mu - test_y)**2).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (machine_learning)",
   "language": "python",
   "name": "machine_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
